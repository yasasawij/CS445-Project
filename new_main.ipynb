{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018d2b24",
   "metadata": {},
   "source": [
    "Juypter notebook focusing on the investigation and use of the MS COCO dataset. This was created for CSU's CS445\n",
    "\n",
    "The goal of this model is to recognize and select cats, dogs, cows, horses, and birds out of a photo\n",
    "\n",
    "With that in mind we will be developing it specifically for instance segmentation\n",
    "\n",
    "Contributers: Tucker Laurence, Yasas Wijesekara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1ada069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import urllib\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe33ce",
   "metadata": {},
   "source": [
    "Global variables used to select conditions about the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "487bedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir='./../annotations'\n",
    "val_data_name='val2017'\n",
    "train_data_name='train2017'\n",
    "\n",
    "val_image_dir = './../datasets/val'\n",
    "train_image_dir = './../datasets/train'\n",
    "\n",
    "# relevant_categories = ['cat', 'dog', 'cow', 'horse', 'bird']\n",
    "relevant_categories = ['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ee98d",
   "metadata": {},
   "source": [
    "This section focuses on the importing and loading of the dataset VIA pycotools.coco, which is the official access tools for MS COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bbb77d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for COCO-style instance segmentation tasks.\n",
    "    if attribute normalize is set to True then any index retrieved sample is normalized\n",
    "\n",
    "     Args:\n",
    "        img_dir (str): Path to directory containing images.\n",
    "        annotation_directory (str): Path to directory with COCO annotation JSON file.\n",
    "        data_name (str): Name suffix for COCO annotation file.\n",
    "        relevant_categories (List[str]): List of category names to include.\n",
    "        transforms (callable, optional): Transformation function to apply to images and targets.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, annotation_directory, data_name, relevant_categories, transforms=None):\n",
    "        # Initialize COCO API and get data during instantiation\n",
    "        annFile = '{}/instances_{}.json'.format(annotation_directory, data_name)\n",
    "        self.coco = COCO(annFile)\n",
    "        self.verbose = True\n",
    "        \n",
    "        self.img_dir = img_dir\n",
    "        self.relevant_categories = relevant_categories\n",
    "        \n",
    "        category_ids, image_ids = self._instantiate_ids()\n",
    "\n",
    "        self.image_ids = image_ids\n",
    "        self.category_ids = category_ids\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Map category_ids to new 1-X labels dynamically\n",
    "        self.category_to_class_dict = {cat_id: idx+1 for idx, cat_id in enumerate(self.category_ids)}\n",
    "        self.class_to_category_dict = {v: k for k, v in self.category_to_class_dict.items()}\n",
    "        \n",
    "        self.normalize = False\n",
    "        self.normal_image_size = (224,224)\n",
    "        \n",
    "        self.image_ids = self._filter_valid_images()\n",
    "\n",
    "    def _instantiate_ids(self):\n",
    "        category_ids = self.coco.getCatIds(catNms=self.relevant_categories)\n",
    "        image_ids = []\n",
    "        \n",
    "        for category in category_ids:\n",
    "            temp_img_ids = self.coco.getImgIds(catIds=category)\n",
    "            image_ids += temp_img_ids\n",
    "        \n",
    "        image_ids = list(set(image_ids))\n",
    "        \n",
    "        print(f\"Found {len(image_ids)} images for {self.relevant_categories}\")\n",
    "        \n",
    "        return category_ids, image_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Number of image samples.\n",
    "        \"\"\"\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Loads a sample image and its corresponding target annotation.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the sample to load.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Image.Image, Dict]: The image and a dictionary containing 'boxes', 'labels', 'masks', and 'image_id'.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Loading item {index}\")\n",
    "            \n",
    "        img_id = self.image_ids[index]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.category_ids, iscrowd=None)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in anns:\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "\n",
    "            original_label = ann['category_id'] #get json label            \n",
    "            labels.append(self.category_to_class_dict[original_label])  # remapped label using dictionary so that labels contains 1-x classes\n",
    "\n",
    "        \n",
    "        #This formatting might need to change\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        if self.normalize:\n",
    "            image, target = self._normalize_sample(image, target, new_size=self.normal_image_size)\n",
    "            \n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "    def _normalize_sample(self, image, target, new_size):\n",
    "        \"\"\"\n",
    "        Resizes image, bounding boxes, and masks to a given size.\n",
    "\n",
    "        Args:\n",
    "            image (PIL.Image or torch.Tensor): The image to normalize.\n",
    "            target (dict): Dictionary containing 'boxes' and 'masks'.\n",
    "            new_size (Tuple[int, int]): Desired (width, height) for output image and masks.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[PIL.Image.Image, Dict]: Resized image and updated target dictionary.\n",
    "        \"\"\"\n",
    "        new_w, new_h = new_size\n",
    "\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            old_h, old_w = image.shape[-2], image.shape[-1]\n",
    "            image = F.to_pil_image(image)\n",
    "        else:\n",
    "            old_w, old_h = image.size\n",
    "\n",
    "        # Resize image\n",
    "        image = image.resize((new_w, new_h))\n",
    "\n",
    "        # Scale boxes\n",
    "        scale_x = new_w / old_w\n",
    "        scale_y = new_h / old_h\n",
    "        boxes = target['boxes']\n",
    "        boxes = boxes.clone()\n",
    "        boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale_x\n",
    "        boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale_y\n",
    "        target['boxes'] = boxes\n",
    "\n",
    "        # Resize masks\n",
    "        resized_masks = []\n",
    "        for mask in target['masks']:\n",
    "            mask_img = Image.fromarray(mask.numpy())\n",
    "            resized_mask = mask_img.resize((new_w, new_h), resample=Image.NEAREST)\n",
    "            resized_masks.append(np.array(resized_mask))\n",
    "        target['masks'] = torch.as_tensor(np.stack(resized_masks), dtype=torch.uint8)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "    def _filter_valid_images(self):\n",
    "        valid_image_ids = []\n",
    "        for img_id in self.image_ids:\n",
    "            try:\n",
    "                img_info = self.coco.loadImgs(img_id)[0]\n",
    "                img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.category_ids, iscrowd=None)\n",
    "                anns = self.coco.loadAnns(ann_ids)\n",
    "                \n",
    "                # Validate there is at least one valid annotation\n",
    "                if len(anns) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Check for valid masks and bounding boxes\n",
    "                for ann in anns:\n",
    "                    _ = self.coco.annToMask(ann)\n",
    "                    _ = ann['bbox']\n",
    "                \n",
    "                valid_image_ids.append(img_id)\n",
    "\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"Skipping image {img_id} due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Filtered dataset down to {len(valid_image_ids)} valid images.\")\n",
    "        return valid_image_ids\n",
    "\n",
    "\n",
    "    \n",
    "    def download_images(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Downloads COCO images into the specified image directory if they are not already present.\n",
    "\n",
    "        Args:\n",
    "            verbose (bool, optional): Whether to print progress messages. Defaults to False.\n",
    "        \"\"\"\n",
    "        current_image_count = 1\n",
    "        total_image_count = len(self)\n",
    "\n",
    "        if not os.path.exists(self.img_dir):\n",
    "            os.makedirs(self.img_dir)\n",
    "\n",
    "        for img_id in self.image_ids:\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            file_name = img_info['file_name']\n",
    "            url = img_info.get('coco_url')\n",
    "\n",
    "            img_count_string = f'{current_image_count}/{total_image_count}'\n",
    "\n",
    "            if url is None:\n",
    "                print(f\"No URL found for image {file_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            save_path = os.path.join(self.img_dir, file_name)\n",
    "            if not os.path.exists(save_path):\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, save_path)\n",
    "                    if verbose: print(f\"{img_count_string} Downloaded {file_name}\")\n",
    "                except Exception as e:\n",
    "                    if verbose:  print(f\"{img_count_string} Failed to download {file_name}: {e}\")\n",
    "            else:\n",
    "                if verbose: print(f\"{img_count_string} Image already exists: {file_name}\")\n",
    "\n",
    "            current_image_count += 1\n",
    "\n",
    "        print(f'Interfaced with {current_image_count-1} unique images')\n",
    "\n",
    "    def show_data_distribution(self):\n",
    "        \"\"\"\n",
    "        Displays a bar chart of the number of instances per category in the dataset.\n",
    "        \"\"\"\n",
    "        all_labels = []\n",
    "        category_labels = []\n",
    "        \n",
    "        for index in range(len(self)):\n",
    "            _, target = self.__getitem__(index)\n",
    "            all_labels.extend(target['labels'].tolist())\n",
    "        \n",
    "        for label in all_labels:\n",
    "            category_labels.append(self.class_to_category_dict[label])\n",
    "\n",
    "        label_counts = Counter(category_labels)\n",
    "        category_names = [self.coco.loadCats([cat_id])[0]['name'] for cat_id in label_counts.keys()]\n",
    "        counts = list(label_counts.values())\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(category_names, counts, color='skyblue')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('Number of Instances')\n",
    "        plt.title('Category Distribution in Dataset')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff1f31",
   "metadata": {},
   "source": [
    "This class is used to show the images in a CoCoSegmentationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e50e9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageVisualizer():\n",
    "    \"\"\"\n",
    "    A helper class to visualize images from a COCO-style segmentation dataset.\n",
    "    Supports displaying plain images, images with bounding boxes, or with segmentation masks.\n",
    "    \"\"\"\n",
    "\n",
    "    def _plot_image_on_ax(self, dataset: CocoSegmentationDataset, ax, index):\n",
    "        \"\"\"\n",
    "        Plots a plain image from the dataset on the given Matplotlib axis.\n",
    "\n",
    "        Args:\n",
    "            dataset (CocoSegmentationDataset): Dataset containing images.\n",
    "            ax (matplotlib.axes.Axes): Axis on which to draw the image.\n",
    "            index (int): Index of the image to display.\n",
    "        \"\"\"\n",
    "        image, _ = dataset[index]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = F.to_pil_image(image)\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"ID: {dataset.image_ids[index]}\", fontsize=8)\n",
    "\n",
    "    def _plot_image_with_boxes_on_ax(self, dataset: CocoSegmentationDataset, ax, index):\n",
    "        \"\"\"\n",
    "        Plots an image with bounding boxes and category labels on the given axis.\n",
    "\n",
    "        Args:\n",
    "            dataset (CocoSegmentationDataset): Dataset containing images and annotations.\n",
    "            ax (matplotlib.axes.Axes): Axis on which to draw.\n",
    "            index (int): Index of the image to display.\n",
    "        \"\"\"\n",
    "        image, target = dataset[index]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = F.to_pil_image(image)\n",
    "\n",
    "        ax.imshow(image)\n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "        category_ids = [dataset.class_to_category_dict[label.item()] for label in labels]\n",
    "        \n",
    "        category_names = [dataset.coco.loadCats([category_id])[0]['name'] for category_id in category_ids]\n",
    "\n",
    "        for box, label, category_name in zip(boxes, labels, category_names):\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1, category_name, color='white', fontsize=6,\n",
    "                    bbox=dict(facecolor='red', edgecolor='none', pad=1))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"ID: {dataset.image_ids[index]}\", fontsize=8)\n",
    "\n",
    "    def _plot_image_with_masks_on_ax(self, dataset: CocoSegmentationDataset, ax, index):\n",
    "        \"\"\"\n",
    "        Plots an image with segmentation masks overlaid on the given axis.\n",
    "\n",
    "        Args:\n",
    "            dataset (CocoSegmentationDataset): Dataset containing images and segmentation masks.\n",
    "            ax (matplotlib.axes.Axes): Axis on which to draw.\n",
    "            index (int): Index of the image to display.\n",
    "        \"\"\"\n",
    "        image, target = dataset[index]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = F.to_pil_image(image)\n",
    "\n",
    "        ax.imshow(image)\n",
    "        masks = target['masks'].numpy()\n",
    "        for mask in masks:\n",
    "            ax.imshow(mask, alpha=0.4)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"ID: {dataset.image_ids[index]}\", fontsize=8)\n",
    "    \n",
    "    def show_images(self, dataset: CocoSegmentationDataset, indices, mode='plain'):\n",
    "        \"\"\"\n",
    "        Displays multiple images from the dataset in a row using the specified visualization mode.\n",
    "\n",
    "        Args:\n",
    "            dataset (CocoSegmentationDataset): Dataset containing images and annotations.\n",
    "            indices (int or list of int): Index/indices of the images to display.\n",
    "            mode (str): Visualization mode. One of 'plain', 'boxes', or 'masks'.\n",
    "        \"\"\"\n",
    "        if type(indices) == int:\n",
    "            indices = [indices]\n",
    "            \n",
    "        cols = len(indices)\n",
    "        fig, axs = plt.subplots(1, cols, figsize=(cols * 4, 4))\n",
    "        \n",
    "        if cols == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        for ax, idx in zip(axs, indices):\n",
    "            if mode == 'plain':\n",
    "                self._plot_image_on_ax(dataset, ax, idx)\n",
    "            elif mode == 'boxes':\n",
    "                self._plot_image_with_boxes_on_ax(dataset, ax, idx)\n",
    "            elif mode == 'masks':\n",
    "                self._plot_image_with_masks_on_ax(dataset, ax, idx)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c1b018ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the visualizer\n",
    "# visualizer = ImageVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fccb7069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Found 177 images for ['dog']\n",
      "Skipping image 72813 due to error: image file is truncated (10 bytes not processed)\n",
      "Filtered dataset down to 176 valid images.\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = CocoSegmentationDataset(train_image_dir, annotation_dir, train_data_name, relevant_categories)\n",
    "val_dataset = CocoSegmentationDataset(val_image_dir, annotation_dir, val_data_name, relevant_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77e2ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.download_images(verbose=True)\n",
    "# val_dataset.download_images(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf93efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_idx = random.randrange(len(val_dataset) - 1)\n",
    "val_dataset.normalize = True\n",
    "# visualizer.show_images(val_dataset, r_idx)\n",
    "# visualizer.show_images(val_dataset, r_idx, mode='boxes')\n",
    "# visualizer.show_images(val_dataset, r_idx, mode='masks')\n",
    "\n",
    "# val_dataset.normalize = True\n",
    "# visualizer.show_images(val_dataset, r_idx)\n",
    "# visualizer.show_images(val_dataset, r_idx, mode='boxes')\n",
    "# visualizer.show_images(val_dataset, r_idx, mode='masks')\n",
    "# val_dataset.normalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16f020f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, target = val_dataset[r_idx]\n",
    "\n",
    "# print(f\"Image shape: {image.shape}\")                 # should be [3, H, W]\n",
    "# print(f\"Boxes: {target['boxes'].shape}\")             # should be [N, 4]\n",
    "# print(f\"Labels: {target['labels']}\")                 # tensor of ints\n",
    "# print(f\"Masks shape: {target['masks'].shape}\")       # [N, H, W]\n",
    "# print(f\"Image ID: {target['image_id']}\")             # tensor([img_id])\n",
    "# print(f\"Mask Values {torch.unique(target['masks'])}\")\n",
    "# print(target['masks'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e9dd9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset.show_data_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904a028",
   "metadata": {},
   "source": [
    "Next steps required for data\n",
    "\n",
    "(Potential, but difficult)\n",
    "Data augmentation: flipping, rotation, color jitter, pixel normalization, etc\n",
    "\n",
    "Datasets needs for pre trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9a07d",
   "metadata": {},
   "source": [
    "MODEL 1 Custom Preprocessing and training/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3fb868f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "609ed83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset.verbose = False\n",
    "# val_dataset.normalize = True\n",
    "\n",
    "# train_loader = DataLoader(val_dataset, \n",
    "#             batch_size=8, \n",
    "#             shuffle=True,\n",
    "#             num_workers=0, \n",
    "#             collate_fn=custom_collate\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee477b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "# import torch\n",
    "\n",
    "# # Model setup\n",
    "# def get_instance_segmentation_model(num_classes):\n",
    "#     # Load a model pre-trained on COCO\n",
    "#     model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "#     # Replace the box predictor\n",
    "#     in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "#     model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "#     # Replace the mask predictor\n",
    "#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "#     hidden_layer = 256\n",
    "#     model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "#         in_features_mask, hidden_layer, num_classes\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "\n",
    "# num_classes = len(relevant_categories) + 1  # +1 for background\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "\n",
    "# # Training setup\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# # Construct optimizer\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# # Training loop\n",
    "# from tqdm import tqdm\n",
    "# num_epochs = 10\n",
    "\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, targets in tqdm(train_loader):\n",
    "#         images = list(img.to(device) for img in images)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += losses.item()\n",
    "\n",
    "#     lr_scheduler.step()\n",
    "#     torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d21944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'epoch': epoch,\n",
    "#     'model_state': model.state_dict(),\n",
    "#     'optimizer_state': optimizer.state_dict(),\n",
    "#     'scheduler_state': lr_scheduler.state_dict()\n",
    "# }, 'checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
